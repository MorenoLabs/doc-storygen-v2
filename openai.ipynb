{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import openai\n",
    "import backoff\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "#OPENAI_API_KEY = os.environ['OPENAI_API_KEY']\n",
    "\n",
    "#client = OpenAI(os.getenv('OPENAI_API_KEY')) pilotblogger\n",
    "\n",
    "client = OpenAI(os.getenv('OPENAI_API_KEY')) #shiprocker \n",
    "\n",
    "\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-4-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"The Los Angeles Dodgers won the World Series in 2020.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Where was it played?\"}\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anthropic\n",
    "\n",
    "clienta = anthropic.Anthropic(\n",
    "    # defaults to os.environ.get(\"ANTHROPIC_API_KEY\")\n",
    ")\n",
    "\n",
    "\n",
    "message = clienta.messages.create(\n",
    "    model=\"claude-3-opus-20240229\",\n",
    "    max_tokens=1000,\n",
    "    temperature=0.0,\n",
    "    system=\"Respond only in Yoda-speak.\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"How are you today?\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(message.content)# Assuming text_block_list is the list containing the TextBlock object\n",
    "text_block = message.content[0]  # Access the TextBlock from the list\n",
    "text = text_block.text  # Get the text from the TextBlock\n",
    "print(text)  # Print the text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(\n",
    "    messages: list[dict[str, str]],\n",
    "    model: str = \"gpt-4\",\n",
    "    max_tokens=500,\n",
    "    temperature=0,\n",
    "    stop=None,\n",
    "    seed=123,\n",
    "    tools=None,\n",
    "    logprobs=None,  # whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the content of message..\n",
    "    top_logprobs=None,\n",
    ") -> str:\n",
    "    params = {\n",
    "        \"model\": model,\n",
    "        \"messages\": messages,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": temperature,\n",
    "        \"stop\": stop,\n",
    "        \"seed\": seed,\n",
    "        \"logprobs\": logprobs,\n",
    "        \"top_logprobs\": top_logprobs,\n",
    "    }\n",
    "    if tools:\n",
    "        params[\"tools\"] = tools\n",
    "\n",
    "    completion = client.chat.completions.create(**params)\n",
    "    return completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSIFICATION_PROMPT = \"\"\"You will be given a headline of a news article.\n",
    "Classify the article into one of the following categories: Technology, Politics, Sports, and Art.\n",
    "Return only the name of the category, and nothing else.\n",
    "MAKE SURE your output is one of the four categories stated.\n",
    "Article headline: {headline}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines = [\n",
    "    \"Tech Giant Unveils Latest Smartphone Model with Advanced Photo-Editing Features.\",\n",
    "    \"Local Mayor Launches Initiative to Enhance Urban Public Transport.\",\n",
    "    \"Tennis Champion Showcases Hidden Talents in Symphony Orchestra Debut\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for headline in headlines:\n",
    "    print(f\"\\nHeadline: {headline}\")\n",
    "    API_RESPONSE = get_completion(\n",
    "        [{\"role\": \"user\", \"content\": CLASSIFICATION_PROMPT.format(headline=headline)}],\n",
    "        model=\"gpt-4\",\n",
    "    )\n",
    "    print(f\"Category: {API_RESPONSE.choices[0].message.content}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Headline: Tech Giant Unveils Latest Smartphone Model with Advanced Photo-Editing Features.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style='color: cyan'>Output token 1:</span> Technology, <span style='color: darkorange'>logprobs:</span> -1.6240566e-06, <span style='color: magenta'>linear probability:</span> 100.0%<br><span style='color: cyan'>Output token 2:</span> Techn, <span style='color: darkorange'>logprobs:</span> -14.171877, <span style='color: magenta'>linear probability:</span> 0.0%<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Headline: Local Mayor Launches Initiative to Enhance Urban Public Transport.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style='color: cyan'>Output token 1:</span> Politics, <span style='color: darkorange'>logprobs:</span> -4.4849444e-06, <span style='color: magenta'>linear probability:</span> 100.0%<br><span style='color: cyan'>Output token 2:</span> Technology, <span style='color: darkorange'>logprobs:</span> -13.156255, <span style='color: magenta'>linear probability:</span> 0.0%<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Headline: Tennis Champion Showcases Hidden Talents in Symphony Orchestra Debut\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style='color: cyan'>Output token 1:</span> Art, <span style='color: darkorange'>logprobs:</span> -0.03364812, <span style='color: magenta'>linear probability:</span> 96.69%<br><span style='color: cyan'>Output token 2:</span> Sports, <span style='color: darkorange'>logprobs:</span> -3.408648, <span style='color: magenta'>linear probability:</span> 3.31%<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from IPython.display import HTML\n",
    "for headline in headlines:\n",
    "    print(f\"\\nHeadline: {headline}\")\n",
    "    API_RESPONSE = get_completion(\n",
    "        [{\"role\": \"user\", \"content\": CLASSIFICATION_PROMPT.format(headline=headline)}],\n",
    "        model=\"gpt-4\",\n",
    "        logprobs=True,\n",
    "        top_logprobs=2,\n",
    "    )\n",
    "    top_two_logprobs = API_RESPONSE.choices[0].logprobs.content[0].top_logprobs\n",
    "    html_content = \"\"\n",
    "    for i, logprob in enumerate(top_two_logprobs, start=1):\n",
    "        html_content += (\n",
    "            f\"<span style='color: cyan'>Output token {i}:</span> {logprob.token}, \"\n",
    "            f\"<span style='color: darkorange'>logprobs:</span> {logprob.logprob}, \"\n",
    "            f\"<span style='color: magenta'>linear probability:</span> {np.round(np.exp(logprob.logprob)*100,2)}%<br>\"\n",
    "        )\n",
    "    display(HTML(html_content))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Passage: In the dimly lit corner of the old library, nestled between the musty, leather-bound volumes of forgotten lore, Helena discovered a small, inconspicuous diary. Its cover was worn, the leather cracked and faded with age, blending almost seamlessly with the row of antique books on the shelf. She had been volunteering in the library, helping the elderly librarian, Mr. Pembroke, catalog its extensive collection. The library had always felt like a sanctuary to her, the silence filled with the whispers of ages past.\n",
      "\n",
      "Mr. Pembroke had warned her about the west wing of the library. \"Don't stray too far into the old collections,\" he would say, his eyes clouding over with a trace of unexplained fear. But Helena, with a curiosity as fierce as the winds on the coast outside, couldn't help herself. It was during one such forbidden expedition that she stumbled upon the hidden diary.\n",
      "\n",
      "The diary seemed to beckon her, its pages a gateway to another time. She opened it gently, the spine creaking as if awakening from a long slumber. The handwriting inside was elegant but hurried, each stroke of the pen etched with urgency. As Helena flipped through the yellowed pages, she found the entries were all penned by a woman named Eliza, who had lived over two centuries ago in the very town Helena called home.\n",
      "\n",
      "Eliza wrote of secret loves, hidden treasures, and unsolved mysteries. Yet, as Helena delved deeper into the diary, the entries grew darker, more desperate. Eliza spoke of a shadow following her, a malevolent force that seemed to grow stronger as she wrote about it. Against her better judgment, Helena felt compelled to read on, the mystery consuming her.\n",
      "\n",
      "Days turned into weeks, and Helena spent more time in the west wing, piecing together Eliza's cryptic entries that suggested the presence of something more sinister lurking within the library. The more she read, the more the air around her seemed to grow colder, the shadows in the library stretching longer.\n",
      "\n",
      "Mr. Pembroke noticed the change in Helena. She looked paler, more withdrawn, shadows under her eyes darkening as if she herself were becoming a part of the library’s gloomy atmosphere. “I told you not to meddle with the old collections,” he warned her, his voice finally revealing the root of his fear. \"The library doesn’t like to give up its secrets.\"\n",
      "\n",
      "One rainy evening, as gray clouds battered the old windows, Helena came across Eliza’s final entry. It was different from the others, scrawled in shaky hand, ink smudged by what looked disturbingly like tear stains. Eliza wrote of a ritual, an attempt to confront the shadow that haunted her, intending to bind it to the library. But her plan had backfired, trapping her soul within the library walls, condemned to linger in the shadows.\n",
      "\n",
      "Helena’s heart raced as she read the chilling last words, \"Whoever learns my truth risks sharing my fate, tethered to this cursed place, forever a part of its haunted lore.\" The diary slipped from her trembling hands, falling to the floor with a thud that echoed through the silent library.\n",
      "\n",
      "From that day on, Helena was never the same. She withdrew from the world, spending her days and nights wandering the stacks of the library, whispering to the shadows. Mr. Pembroke, heartbroken at her transformation, resigned himself to the somber duty of watching over her, just as he had promised to watch over the library’s secrets.\n",
      "\n",
      "In the end, Helena, too, became one of the library’s many secrets - a forlorn figure occasionally glimpsed between the shelves, her whispers carried in the dusty silence, another tragic soul lost to the library’s ancient curse.\n",
      "Scoring Results: No.\n",
      "Log Probabilities: ChoiceLogprobs(content=[ChatCompletionTokenLogprob(token='No', bytes=[78, 111], logprob=-0.04050042, top_logprobs=[TopLogprob(token='No', bytes=[78, 111], logprob=-0.04050042), TopLogprob(token='no', bytes=[110, 111], logprob=-3.2280004)]), ChatCompletionTokenLogprob(token='.', bytes=[46], logprob=-0.009775686, top_logprobs=[TopLogprob(token='.', bytes=[46], logprob=-0.009775686), TopLogprob(token='<|end|>', bytes=None, logprob=-4.6347756)])])\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'HTML' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 64\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, logprob \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(top_two_logprobs, start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     59\u001b[0m     html_content \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     60\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<span style=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcolor: cyan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m>Output token \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:</span> \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlogprob\u001b[38;5;241m.\u001b[39mtoken\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     61\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<span style=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcolor: darkorange\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m>logprobs:</span> \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlogprob\u001b[38;5;241m.\u001b[39mlogprob\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<span style=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcolor: magenta\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m>linear probability:</span> \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mround(np\u001b[38;5;241m.\u001b[39mexp(logprob\u001b[38;5;241m.\u001b[39mlogprob)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m,\u001b[38;5;241m2\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%<br>\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     63\u001b[0m     )\n\u001b[0;32m---> 64\u001b[0m display(\u001b[43mHTML\u001b[49m(html_content))\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'HTML' is not defined"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import os\n",
    "\n",
    "from openai import OpenAI\n",
    "import openai\n",
    "import backoff\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "#OPENAI_API_KEY = os.environ['OPENAI_API_KEY']\n",
    "\n",
    "client = OpenAI(api_key=os.getenv('OPENAI_API_KEY')) #ship\n",
    "\n",
    "# Setup the OpenAI API key\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')  # Ensure you have this set in your environment variables\n",
    "\n",
    "def generate_story_passage(context):\n",
    "    \"\"\" Generate a story passage based on the given context using OpenAI's API. \"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "    model=\"gpt-4-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Generate a story passage based on the given context\"},\n",
    "        {\"role\": \"user\", \"content\": context}]\n",
    "    )   \n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "def score_passage(passage, context):\n",
    "    \"\"\" Score the generated passage for coherence, relevance, and commentary. \"\"\"\n",
    "    scoring_prompt = f\"Review the following passage: {passage} Based on the initial context: {context}\\n\\n\"\n",
    "    scoring_prompt += \"Will the story have a happy outcome? YOU MUST ANSWER WITH ONLY EITHER yes' or 'no'.\\n\\n\"\n",
    "\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "    model=\"gpt-4-turbo\",\n",
    "     logprobs=True,\n",
    "    top_logprobs=2,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": scoring_prompt}]\n",
    "    )   \n",
    "\n",
    "    return response.choices[0].message.content.strip(), response.choices[0].logprobs\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    context = \"The old library held secrets beyond the books on its shelves. The protagonist uncovers a hidden diary behind the ancient texts. a sad end will befall the protagonist.\"\n",
    "    passage = generate_story_passage(context)\n",
    "    print(\"Generated Passage:\", passage)\n",
    "\n",
    "    score, log_probs = score_passage(passage, context)\n",
    "    print(\"Scoring Results:\", score)\n",
    "    print(\"Log Probabilities:\", log_probs)\n",
    "\n",
    "    top_two_logprobs = log_probs.content[0].top_logprobs\n",
    "    html_content = \"\"\n",
    "    for i, logprob in enumerate(top_two_logprobs, start=1):\n",
    "        html_content += (\n",
    "            f\"<span style='color: cyan'>Output token {i}:</span> {logprob.token}, \"\n",
    "            f\"<span style='color: darkorange'>logprobs:</span> {logprob.logprob}, \"\n",
    "            f\"<span style='color: magenta'>linear probability:</span> {np.round(np.exp(logprob.logprob)*100,2)}%<br>\"\n",
    "        )\n",
    "    display(HTML(html_content))\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChoiceLogprobs(content=[ChatCompletionTokenLogprob(token='No', bytes=[78, 111], logprob=-0.04050042, top_logprobs=[TopLogprob(token='No', bytes=[78, 111], logprob=-0.04050042), TopLogprob(token='no', bytes=[110, 111], logprob=-3.2280004)]), ChatCompletionTokenLogprob(token='.', bytes=[46], logprob=-0.009775686, top_logprobs=[TopLogprob(token='.', bytes=[46], logprob=-0.009775686), TopLogprob(token='<|end|>', bytes=None, logprob=-4.6347756)])])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TopLogprob(token='No', bytes=[78, 111], logprob=-0.04050042),\n",
       " TopLogprob(token='no', bytes=[110, 111], logprob=-3.2280004)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_two_logprobs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Probabilities: ChoiceLogprobs(content=[ChatCompletionTokenLogprob(token='Yes', bytes=[89, 101, 115], logprob=-3.094816e-05, top_logprobs=[TopLogprob(token='Yes', bytes=[89, 101, 115], logprob=-3.094816e-05), TopLogprob(token='No', bytes=[78, 111], logprob=-11.0625305)]), ChatCompletionTokenLogprob(token='.', bytes=[46], logprob=-0.46241638, top_logprobs=[TopLogprob(token='.', bytes=[46], logprob=-0.46241638), TopLogprob(token='<|end|>', bytes=None, logprob=-0.9936664)])])\n"
     ]
    }
   ],
   "source": [
    "print(\"Log Probabilities:\", log_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TopLogprob(token='Yes', bytes=[89, 101, 115], logprob=-3.094816e-05),\n",
       " TopLogprob(token='No', bytes=[78, 111], logprob=-11.0625305)]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_probs.content[0].top_logprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.choices[0].message.content.strip()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
